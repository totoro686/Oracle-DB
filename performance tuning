Optimization tips
This will help to optimize the performance of plsql/sql and also for ease maintenance of the code. 
1.1)	Be aware of your tables’ expected content, data volume and use within the application.
Reason: Design your table properly by using primary key, constraint, index, partition to define your 
preferred access path when you use it.
1.2)	Be careful of using the bind variable (PL/SQL variables, constants and parameters) because it could slow down your performance 
Use literal value if possible in OLAP environment. 
Reason: The optimizer will use bind variable peeking to generate the execution plan and if the data in table is skew this may cause a 
bad execution plan. To use literal value in the SQL can avoid this. Be notes, this is an OLAP environment, so time saving from getting 
the right/correct execution plan is much more compared to the hard parse. 
1.3)	Use LIKE “%...” with caution
Try to avoid the pattern that begins with the wildcard character “%”.
Reason: If there is an index for this column, the statement using like “%..” will not be able to use the index.
1.4)	Using BULK COLLECT whenever you need to put data in the ARRAY. For example, load ing 100 millions of 
rows into FACT table in our DATA WAREHOUSE, if we load 1 million as one time, it will with the following error:
ERROR at line 1:  
       ORA-04030: out of process memory when trying to allocate 16408 bytes (koh-kghu  call ,pmucalm coll)  
       ORA-06512: at "VVA.BULKLOAD", line 66  
       ORA-06512: at line 1  We got the same error even with 1 million rows.  
       We do have the following configuration:  
             SGA - 8.2 GB  
             PGA 
              - Aggregate Target - 3GB  
              - Current Allocated - 439444KB (439 MB)  
              - Maximum allocated - 2695753 KB (2.6 GB)  
              - Temp Table Space - 60.9 GB (Total)  
              - 20 GB (Available approximately)  
      I think we do have more than enough memory to process the 1 million rows!!  
Reason: This will help you gain better performance by avoiding too frequently engine switch. No standard to 
set the limit value, it depends on the size of the PGA.
 
1.5)	Always Close locally opened cursor
Reason: Any cursors left open can consume additional system Global Area memory space within the database instance, potentially in both the shared and private SQL pools. Furthermore, failure to explicitly 
close cursors may also cause the owning session to exceed its maximum limit of open cursors and 
trigger the Oracle Error “ORA-01000 maximum open cursors exceeded”. 
 
1.6)	 Try to use CASE rather than DECODE
Reason: DECODE is an old function that has been replaced by the easier-to-understand and more common CASE function. 

1.7)	 Try to use the function to implement the logic that is commonly used/shared.
This will help the developer to avoid writing the same logic again and again so will reduce the chance of making mistakes also reduce 
the development time. But using this kind of design ONLY in case the logic is COMMON and can be re-used because if not there are 
several flaws for this kind of design:
1. To the code readers, it makes the logic trace harder because it required developer to go through the code up and down to understand 
the logic so it waste lots of time to map the parameter and the real value when doing debug.
2. To the code writer, it takes more time to do coding this way (if putting no-repeat logic that cannot be shared in this way).
3. Performance, although the function is in the package, it still requires some extra work compared to doing the logic directly.
 
1.8)	Learn to use the WITH clause. 
WITH clause can be used to reduce repetition and simplify complex SQL statements. 
The advantage is that repeated references to the sub query may be more efficient as the data is easily retrieved from the temporary 
table, rather than being required by each reference. You should assess the performance implications of the WITH clause on a case-by-case
basis, if the result set from the sub query is very large, it’s may be not a good idea to do so, the alternative is to create an 
intermediate table and store the data permanently for others to use.
Step 1. Use the WITH..AS to materialize the sub query result set
Step 2. Join it as a normal table 
 
 
1.9)	The usage of foreign keys
1. When creating foreign keys on a table, you must create an index on the foreign keys columns
2. Be careful when creating foreign keys , we shouldn't have loop dependency using foreign keys. 
A loop dependency example: 
Table A refer to Table B, and Table C refer to table B, also Table C refer to A 
A dead lock happen if we have loop dependency like this.

1.10)  before loading 1GB into a table, we usually disable index, because Oracle DB will also maintain the index
 in this process. After disable the index, we can improve preformance.

1.11) table compression

1.12) Usw ANSI way to join tables in the query

Although in most cases the execution paths for equivalent ANSI and Oracle syntax queries will be the same (exception being the FULL OUTER JOIN), the advantage of adopting the ANSI JOIN syntax is code readability and maintainability. It can be argued that using ANSI JOIN syntax increases the
amount of code written, however the gains that are made in code readability far out-weigh this disadvantage as it allows one to separate the logic of joining datasets from the logic of filtering of datasets from the logic of filtering of datasets.

1.13)  There are two basic methods that Oracle uses for inserts. 
         The slow way is usually called conventional. With this mechanism, the data goes through the buffer cache, empty space in existing blocks is reused, undo is generated for all data and metadata changes, and redo is generated for all changes by default. This is a lot of work. That's why I call it the slow way.

         The fast way is also called direct path. This mechod does not look for space in existing blocks, it just starts inserting data above the hight water mark. It protects the data dictionary with undo and redo for metedata changes but it generates no undo for data changes. It can also avoid redo generation for data chnages in some cases. Direct path inserts can be invoked by using the APPEND hint.
             There are a few issues with the fast approach, however.
             1) Only one direct path write can be occurring on a table at any given time.
             2) Data will be inserted above the high water mark, so any available space in the blocks below the high water mark will not be used by the direct path inserts.
             3) The session that performs the insert append can't do anythin with the table after the insert until a commit or rollback is issued.
             4) Object types, indexed orgnized table, etc are not supported.
             5) referential constriants are not supported.

1.14) semi-join - is join between two sets of data where rows from the first set are returned, based on the presence or absence of at least one 
                  matching row in the other set.
      anti-join - returns rowd from the left side of the predicate for which there are no corresponding rows on the right side of the predicate. 
                  It eturns rows that fail to match the subquery on the right side.   
      inline view - An inline view is a SELECT statement in the FROM-clause of another SELECT statement. In-line views are commonly used to 
                    simplify complex queries by removing join operations and condensing several separate queries into a single query. 
               Example for inline view:     
               SELECT * 
                  FROM ( SELECT deptno, count(*) emp_count
                         FROM emp
                         GROUP BY deptno ) emp,
                       dept
                WHERE dept.deptno = emp.deptno;         

1.15）DML error logging
      This feature provides a mechanism for preventing your one million row insert from failing because a few rows had problems.
         1). Create the Error Log table using DBMA_ERRLOG.CREATE_ERROR_LOG
         2). Specify the LOG ERRORS clause on the INSERT.

      In real life, the Reject Limit would generally be set to UNLIMITED.   

      The DBMS_ERRLOG package provides a procedure that enables you to create an error logging table so that DML operations can continue after encountering errors rather than abort and roll back. This enables you to save time and system resources.

      CREATE_ERROR_LOG Procedure：This procedure creates the error logging table needed to use the DML error logging capability. LONG, CLOB, BLOB, BFILE, and ADT datatypes are not supported in the columns.

      INSERT /*+ APPEND */ INTO table_name SUBPARTITION( subpartition_name ) 
        SELECT * FROM table_name WHERE business_date= '2016-01-01'' AND site_code= 'CN' LOG ERRORS REJECT LIMIT UNLIMITED;


reference link: http://www.voidcn.com/blog/fsldg/article/p-1186874.html
                http://docs.oracle.com/database/121/ARPLS/d_errlog.htm#ARPLS66324
                http://docs.oracle.com/database/121/ADMIN/tables.htm#ADMIN-GUID-36DB026B-4702-477A-92C4-EA2795D2B37F

1.16) In other words, why do we have to wait for the enq: RO – fast object reuse wait event 
      (and in 11.2 the enq: CR – block range reuse ckpt wait) when dropping & truncating segments?
      
      Imagine this:
 

    You have a large buffer cache and you drop table A without checkpointing the dirty buffers.  
    Immediately after the drop succeeds (some buffers are still dirty in cache) some other segment (table B) reuses that space for itself and writes stuff into it.
    A few seconds later, DBWR wakes up to find & write some dirty buffers to disk (anything it finds from its lists). As there are some old & dirty blocks of table A still in the cache, they get written to disk too, overwriting some of the new table B blocks!

  

So, this is one reason why you should checkpoint the blocks to disk before dropping (or truncating) a segment. Of course you might ask that why doesn’t DBWR just check whether the dirty buffer is part of an existing object or a dropped one when it walks through its dirty list? It could just discard the dirty buffers of dropped objects it finds. It would be doable – but I also think it would get quite complex. DBWR is a low level background proces, understanding the cache layer and dealing with physical datablocks in a file# = X block offset = Y. It doesn’t really know anything about the segments/objects which use these blocks. If it should start checking for logical existence of an object, it would have to start running code to access (a much higher level concept) data dictionary cache – and possibly query data dictionary tables via recursive calls, etc, so making it much more complicated.
 
So, this logic may just be matter of implementation, it’d be too complex to implement such selective discarding of dirty buffers, based on a higher-level concept of existence of a segment or object. Dropping and truncating tables so frequently, that these waits become a serious problem (consuming significant % of response time) indicate a design problem anyway. For example, former SQL server developers creating multiple temporary tables in Oracle – for breaking a complex query down into smaller parts, just like they had been doing it in SQL Server.
      Solution: alter system flush buffer_cache;
      reference link: http://blog.tanelpoder.com/2011/07/06/what-is-the-purpose-of-segment-level-checkpoint-before-droptruncate-of-a-table/


How to improve the performance of SQL
1. Do not use the set operator UNION if the objective can be achieved through an UNION ALL. UNION incurs an extra sort operation 
which can be avoided.
2. Select ONLY those columns in a query which are required. Extra columns which are not actually used, incur more I/O on the database 
and increase network traffic.
3. Do not use the keyword DISTINCT if the objective can be achieved otherwise. DISTINCT incurs an extra sort operation and therefore 
slows your queries down.
4. If it is required to use a composite index, try to use the “Leading” column in the “WHERE” clause. Though Index skip scan is 
possible, it incurs extra cost in creating virtual indexes and may not be always possible depending on the cardinality of the leading 
columns.
5. There should not be any Cartesian product in the query unless there is a definite requirement to do so. I know this is a silly point 
but we all have done this mistake at one point.
6. Wherever multiple tables are used, always refer to a column by either using an alias or using the fully qualified name. Do not 
leave the guess work for Oracle.
7. SQL statements should be formatted consistently (e.g the keywords should be in CAPS only) to aid readability. Now, this is not a 
performance tip really. However, it’s important and part of the practices.
8. If possible use bind variables instead of constant/literal values in the predicate filter conditions to reduce repeated parsing of 
the same statement.
9. Use meaningful aliases for tables/views
10. When writing sub-queries make use of the EXISTS operator where possible as Oracle knows that once a match has been found it can 
stop and avoid a full table scan (it does a SEMI JOIN).
11. If the selective predicate is in the sub query, then use IN.
12. If the selective predicate is in the parent query, then use EXISTS.
13. Do not modify indexed columns with functions such as RTRIM, TO_CHAR, UPPER, TRUNC as this will prevent the optimizer from 
identifying the index. If possible perform the modification on the constant side of the condition. If the indexed column is usually 
accessed through a function (e.g NVL), consider creating a function based index.
14. Try to use an index if less than 5% of the data needs to be accessed from a data set. The exception is a small table (a few hundred 
rows) which is usually best accessed through a FULL table scan irrespective of the percentage of data required.
15. Use equi-joins whenever possible, they improve SQL efficiency
16. Avoid the following kinds of complex expressions:
NVL (col1,-999) = ….
TO_DATE(), TO_NUMBER(), and so on 
These expressions prevent the optimizer from assigning valid cardinality or selectivity estimates and can in turn affect the overall 
plan and the join method
17. It is always better to write separate SQL statements for different tasks, but if you must use one SQL statement, then you can make 
a very complex statement slightly less complex by using the UNION ALL operator
18. Joins to complex views are not recommended, particularly joins from one complex view to another. Often this results in the entire 
view being instantiated, and then the query is run against the view data
19. Querying from a view requires all tables from the view to be accessed for the data to be returned. If that is not required, then 
do not use the view. Instead, use the base table(s), or if necessary, define a new view.
20. While querying on a partitioned table try to use the partition key in the “WHERE” clause if possible. This will ensure partition 
pruning.
21. Consider using the PARALLEL hint (only when additional resources can be allocated) while accessing large data sets.
22. Avoid doing an ORDER BY on a large data set especially if the response time is important.
23. Consider changing the OPTIMIZER MODE to FIRST_ROWS(n) if the response time is important. The default is ALL_ROWS which gives 
better throughput.
24. Use CASE statements instead of DECODE (especially where nested DECODEs are involved) because they increase the readability of 
the query immensely.
25. Do not use HINTS unless the performance gains clear.
26. Check if the statistics for the objects used in the query are up to date. If not, use the DBMS_STATS package to collect the same.
27. It is always good to understand the data both functionally and it’s diversity and volume in order to tune the query. Selectivity 
(predicate) and Cardinality (skew) factors have a big impact on query plan. Use of Statistics and Histograms can drive the query towards
a better plan.
28. Read explain plan and try to make largest restriction (filter) as the driving site for the query, followed by the next largest, this 
will minimize the time spent on I/O and execution in subsequent phases of the plan.
29. If Query requires quick response rather than good throughput is the objective, try to avoid sorts (group by, order by, etc.). For 
good throughput, optimizer mode should be set to ALL ROWS.
30. Queries tend to perform worse as they are due to volume increase, structural changes in the database and application, upgrades etc. 
Use Automatic Workload Repository (AWR) and Automatic Database Diagnostic Monitor (ADDM) to better understand change in execution plan 
and throughput of top queries over a period of time.
31. SQL Tuning Advisor and SQL Access Advisor can be used for system advice on tuning specific SQL and their join and access paths, 
however, advice generated by these tools may not be always applicable (point 28).
32. SQL Access paths for joins are an component determining query execution time. Hash Joins are preferable when 2 large tables need 
to be joined. Nested loops make work better when a large table is joined with a small table.



